{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "421365f2",
      "metadata": {
        "id": "421365f2"
      },
      "source": [
        "# 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "4b155c87",
      "metadata": {
        "id": "4b155c87"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "from time import time\n",
        "\n",
        "X_train = np.load('train_data.npy')\n",
        "X_test = np.load('test_data.npy')\n",
        "y_train = np.load('train_label.npy')\n",
        "y_test = np.load('test_label.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "ec8636bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec8636bc",
        "outputId": "896b85df-9b07-442e-f53c-ef9f2be4c586"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 128), (10000, 128), (50000, 1), (10000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c83d0ca",
      "metadata": {
        "id": "0c83d0ca"
      },
      "source": [
        "# 2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "train_data_scaled = scaler.fit_transform(X_train)\n",
        "test_data_scaled = scaler.transform(X_test)\n",
        "\n",
        "train_data_min, train_data_max = train_data_scaled.min(), train_data_scaled.max()\n",
        "test_data_min, test_data_max = test_data_scaled.min(), test_data_scaled.max()\n",
        "\n",
        "train_data_min, train_data_max, test_data_min, test_data_max\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBAPZ6nGF_6_",
        "outputId": "49971c0e-4a76-4c50-fc73-9c95af9dc0bb"
      },
      "id": "HBAPZ6nGF_6_",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 1.0000000000000002, -0.2341860850703863, 1.1540823219473646)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "104d5337",
      "metadata": {
        "id": "104d5337"
      },
      "source": [
        "# 3. Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce68dda7",
      "metadata": {
        "id": "ce68dda7"
      },
      "source": [
        "## 3.1 Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "b9bf35f8",
      "metadata": {
        "id": "b9bf35f8"
      },
      "outputs": [],
      "source": [
        "class Activation(object):\n",
        "    def __relu(self, x):\n",
        "        return np.where(x >= 0, x, 0)\n",
        "\n",
        "    def __relu_derivative(self, a):\n",
        "        return np.where(a >= 0, 1, 0)\n",
        "\n",
        "    def __softmax(self, x):\n",
        "        x_exponent = np.exp(x - np.max(x, axis = -1, keepdims = True))\n",
        "        return x_exponent / np.sum(x_exponent, axis = -1, keepdims = True)\n",
        "\n",
        "    def __init__(self, activation = 'relu'):\n",
        "\n",
        "        if activation == 'relu':\n",
        "            self.function = self.__relu\n",
        "            self.function_derivative = self.__relu_derivative\n",
        "        elif activation == 'softmax':\n",
        "            self.function = self.__softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb256d68",
      "metadata": {
        "id": "bb256d68"
      },
      "source": [
        "## 3.2 Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "7e87f565",
      "metadata": {
        "id": "7e87f565"
      },
      "outputs": [],
      "source": [
        "class Layer(object):\n",
        "\n",
        "    def __init__(self, n_in, n_out, optimizer, activation = 'relu'):\n",
        "        self.input = None\n",
        "        self.output_before_activation = None\n",
        "        self.output = None\n",
        "\n",
        "        self.activation = Activation(activation).function\n",
        "        if activation == 'softmax':\n",
        "            self.activation_derivative = None\n",
        "        else:\n",
        "            self.activation_derivative = Activation(activation).function_derivative\n",
        "\n",
        "        self.Weight = np.random.uniform(\n",
        "            low = -np.sqrt(6 / (n_in + n_out)),\n",
        "            high = np.sqrt(6 / (n_in + n_out)),\n",
        "            size = (n_in, n_out)\n",
        "        )\n",
        "        self.bias = np.zeros(n_out,)\n",
        "\n",
        "        self.optimizer_weight = copy.copy(optimizer)\n",
        "        self.optimizer_bias = copy.copy(optimizer)\n",
        "\n",
        "    def forward(self, input, train = True):\n",
        "\n",
        "        self.input = input\n",
        "        self.output_before_activation = np.dot(input, self.Weight) + self.bias\n",
        "        self.output = self.activation(self.output_before_activation)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, delta):\n",
        "\n",
        "        if self.activation_derivative:\n",
        "            delta = delta * self.activation_derivative(self.output_before_activation)\n",
        "\n",
        "        grad_weight = np.dot(self.input.T, delta)\n",
        "        grad_bias = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "        self.Weight = self.optimizer_weight.update(self.Weight, grad_weight)\n",
        "        self.bias = self.optimizer_bias.update(self.bias, grad_bias)\n",
        "\n",
        "        delta = np.dot(delta, self.Weight.T)\n",
        "\n",
        "        return delta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b8bd0c",
      "metadata": {
        "id": "28b8bd0c"
      },
      "source": [
        "## 3.3 Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "e6a63e89",
      "metadata": {
        "id": "e6a63e89"
      },
      "outputs": [],
      "source": [
        "class DropoutLayer(object):\n",
        "\n",
        "    def __init__(self, drop_prob: float = 0.5):\n",
        "        self.drop_prob = drop_prob\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, X: np.ndarray, train: bool = True) -> np.ndarray:\n",
        "        if train:\n",
        "            self.mask = np.random.rand(*X.shape) >= self.drop_prob\n",
        "            return X * self.mask\n",
        "        else:\n",
        "            return X * (1 - self.drop_prob)\n",
        "\n",
        "    def backward(self, delta: np.ndarray) -> np.ndarray:\n",
        "        return delta * self.mask if self.mask is not None else delta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee80f568",
      "metadata": {
        "id": "ee80f568"
      },
      "source": [
        "## 3.4 BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "f2e4ea82",
      "metadata": {
        "id": "f2e4ea82"
      },
      "outputs": [],
      "source": [
        "class BatchNormalization(object):\n",
        "\n",
        "    def __init__(self, gamma, beta, optimizer, momentum = 0.9):\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.mean = 0\n",
        "        self.var = 1\n",
        "        self.gamma_optimizer = copy.copy(optimizer)\n",
        "        self.beta_optimizer = copy.copy(optimizer)\n",
        "\n",
        "    def forward(self, X, train = True):\n",
        "\n",
        "        if self.mean is None:\n",
        "            self.mean = np.mean(X, axis = 0)\n",
        "            self.var = np.var(X, axis = 0)\n",
        "\n",
        "        if train:\n",
        "            mean = np.mean(X, axis = 0)\n",
        "            self.mean = self.momentum * self.mean + (1 - self.momentum) * mean\n",
        "            var = np.var(X, axis = 0)\n",
        "            self.var = self.momentum * self.var + (1 - self.momentum) * var\n",
        "        else:\n",
        "            mean = self.mean\n",
        "            var = self.var\n",
        "\n",
        "        self.X_minus_mean = X - mean\n",
        "        self.std = np.sqrt(var + 1e-6)\n",
        "        self.X_norm = self.X_minus_mean / self.std\n",
        "        output = self.gamma * self.X_norm + self.beta\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, delta):\n",
        "\n",
        "        gamma_old = self.gamma\n",
        "\n",
        "        gamma_grad = np.sum(delta * self.X_norm, axis = 0)\n",
        "        beta_grad = np.sum(delta, axis = 0)\n",
        "\n",
        "        self.gamma = self.gamma_optimizer.update(self.gamma, gamma_grad)\n",
        "        self.beta = self.beta_optimizer.update(self.beta, beta_grad)\n",
        "\n",
        "        dX_norm = delta * gamma_old\n",
        "        dvar = np.sum(dX_norm * self.X_minus_mean, axis = 0) * (-0.5) * (self.var + 1e-6)**(-3/2)\n",
        "        dmean = np.sum(dX_norm * (1/self.std), axis = 0) + dvar * (1/delta.shape[0]) * np.sum(-2 * self.X_minus_mean, axis = 0)\n",
        "        delta = (dX_norm * (1/self.std)) + (dmean / delta.shape[0]) + (dvar * 2 / delta.shape[0] * self.X_minus_mean)\n",
        "\n",
        "        return delta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e030a362",
      "metadata": {
        "id": "e030a362"
      },
      "source": [
        "## 3.5 Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "3784542a",
      "metadata": {
        "id": "3784542a"
      },
      "outputs": [],
      "source": [
        "class Optimizer(object):\n",
        "\n",
        "    def __init__(self, lr = 0.001, momentum = 0.9, weight_decay: float = 1e-2):\n",
        "\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.weight_decay = weight_decay\n",
        "        self.grad = None\n",
        "\n",
        "    def update(self, weight, delta):\n",
        "\n",
        "        if self.grad is None:\n",
        "            self.grad = np.zeros(weight.shape)\n",
        "\n",
        "        self.grad = self.momentum * self.grad + (1 - self.momentum) * delta\n",
        "        weight = weight * (1 - self.weight_decay) - self.lr * self.grad\n",
        "\n",
        "        return weight"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fef8df57",
      "metadata": {
        "id": "fef8df57"
      },
      "source": [
        "## 3.6 Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "c69346d5",
      "metadata": {
        "id": "c69346d5"
      },
      "outputs": [],
      "source": [
        "class MLP(object):\n",
        "\n",
        "    def __init__(self, n_in, n_out, layers, optimizer, activation, BN=False, Dropout=False, dropout_prob=None):\n",
        "\n",
        "        self.layers = []\n",
        "        self.activation = activation\n",
        "        self.optimizer = optimizer\n",
        "        self.lr = self.optimizer.lr\n",
        "        self.n_out = n_out\n",
        "\n",
        "        self.layers.append(Layer(n_in, layer[0], optimizer, activation[0]))\n",
        "        if Dropout:\n",
        "            self.layers.append(DropoutLayer(dropout_prob[0]))\n",
        "        if BN:\n",
        "            self.layers.append(BatchNormalization(np.ones((1, layer[0])), np.zeros((1, layer[0])), optimizer))\n",
        "\n",
        "        for i in range(1, len(layer)):\n",
        "            self.layers.append(Layer(layer[i-1], layer[i], optimizer, activation[i]))\n",
        "            if Dropout:\n",
        "                self.layers.append(DropoutLayer(dropout_prob[i]))\n",
        "            if BN:\n",
        "                self.layers.append(BatchNormalization(np.ones((1, layer[i])), np.zeros((1, layer[i])), optimizer))\n",
        "\n",
        "        self.layers.append(Layer(layer[-1], n_out, optimizer, activation[-1]))\n",
        "\n",
        "    def CE_loss(self, y, predict_y):\n",
        "\n",
        "        y_onehot = np.eye(self.n_out)[y].reshape(-1, self.n_out)\n",
        "        predict_y = np.clip(predict_y, 1e-15, 1 - 1e-15)\n",
        "        loss = -np.sum(np.multiply(y_onehot, np.log(predict_y)))\n",
        "        delta = predict_y - y_onehot\n",
        "        return loss, delta\n",
        "\n",
        "    def forward(self, input, train = True):\n",
        "\n",
        "        output = input\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output, train)\n",
        "        return output\n",
        "\n",
        "    def backward(self, delta):\n",
        "\n",
        "        for layer in reversed(self.layers):\n",
        "            delta = layer.backward(delta)\n",
        "\n",
        "    def fit(self, X, y, epochs = 100, batch_size = 100, print_per = 50):\n",
        "\n",
        "        loss_list = []\n",
        "        accuracy_list = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            if epoch == int(epochs*1/3):\n",
        "                self.lr = self.lr / 5\n",
        "                self.optimizer = Optimizer(lr = self.lr)\n",
        "            elif epoch == int(epochs*2/3):\n",
        "                self.lr = self.lr / 5\n",
        "                self.optimizer = Optimizer(lr = self.lr)\n",
        "\n",
        "            loss_temp = 0\n",
        "            predict_y_all_batch = []\n",
        "\n",
        "            start = time()\n",
        "\n",
        "            for index in np.arange(0, X.shape[0], batch_size):\n",
        "                X_batch = X[index: min(index+batch_size, X.shape[0])]\n",
        "                y_batch = y[index: min(index+batch_size, X.shape[0])]\n",
        "\n",
        "                predict_y = self.forward(X_batch)\n",
        "\n",
        "                loss, delta = self.CE_loss(y_batch, predict_y)\n",
        "\n",
        "                self.backward(delta)\n",
        "\n",
        "                loss_temp += loss\n",
        "                predict_y_all_batch.extend(predict_y)\n",
        "\n",
        "            predict_y_all_batch = np.array(predict_y_all_batch)\n",
        "\n",
        "            loss_list.append(loss_temp / X.shape[0])\n",
        "            predict_y = np.argmax(predict_y_all_batch, axis = 1).reshape(-1,1)\n",
        "            accuracy = np.sum(predict_y == y, axis = 0) / X.shape[0]\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if (epoch + 1) % print_per == 0:\n",
        "                print(\"Epoch: %d\\tTime: %.2fs\\tLoss: %.5f\\tAccuracy: %.2f%%\" % (epoch+1, time()-start, loss_list[-1], accuracy_list[-1]*100))\n",
        "\n",
        "        return loss_list, accuracy_list\n",
        "\n",
        "    def predict(self, X, y):\n",
        "\n",
        "        predict_y = self.forward(X, train = False)\n",
        "\n",
        "        loss, _ = self.CE_loss(y, predict_y)\n",
        "\n",
        "        accuracy = np.sum(np.argmax(predict_y, axis = 1).reshape(-1,1) == y, axis = 0) / X.shape[0]\n",
        "\n",
        "        print(\"Loss: %.5f\\tAccuracy:%.2f%%\" % (loss, accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0eff823",
      "metadata": {
        "id": "c0eff823"
      },
      "source": [
        "# 4. Adjust Parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Optimizer(lr = 0.001, momentum = 0.8, weight_decay = 1e-3)\n",
        "\n",
        "n_in = X_train.shape[1]\n",
        "n_out = len(np.unique(y_train))\n",
        "\n",
        "layer = [256, 512]\n",
        "\n",
        "activation = ['relu', 'relu', 'softmax']\n",
        "model = MLP(n_in, n_out, layer, optimizer, activation, BN = True, Dropout = True, dropout_prob = [0.2, 0.2])\n",
        "\n",
        "loss, accuracy = model.fit(X_train, y_train, epochs = 30, batch_size = 500, print_per = 3)\n",
        "\n",
        "print()\n",
        "\n",
        "model.predict(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFXWm15fIlN2",
        "outputId": "39055ef1-8a07-4ac1-90c7-b5713e2ba0bc"
      },
      "id": "vFXWm15fIlN2",
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-91-a1e3bc24fb30>:86: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"Epoch: %d\\tTime: %.2fs\\tLoss: %.5f\\tAccuracy: %.2f%%\" % (epoch+1, time()-start, loss_list[-1], accuracy_list[-1]*100))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3\tTime: 6.44s\tLoss: 1.49417\tAccuracy: 46.65%\n",
            "Epoch: 6\tTime: 5.05s\tLoss: 1.36256\tAccuracy: 51.05%\n",
            "Epoch: 9\tTime: 5.12s\tLoss: 1.29693\tAccuracy: 53.67%\n",
            "Epoch: 12\tTime: 6.39s\tLoss: 1.25332\tAccuracy: 55.06%\n",
            "Epoch: 15\tTime: 4.98s\tLoss: 1.23128\tAccuracy: 55.83%\n",
            "Epoch: 18\tTime: 4.95s\tLoss: 1.21056\tAccuracy: 56.75%\n",
            "Epoch: 21\tTime: 6.34s\tLoss: 1.19408\tAccuracy: 57.20%\n",
            "Epoch: 24\tTime: 6.28s\tLoss: 1.18446\tAccuracy: 57.88%\n",
            "Epoch: 27\tTime: 4.98s\tLoss: 1.16928\tAccuracy: 58.31%\n",
            "Epoch: 30\tTime: 5.02s\tLoss: 1.16276\tAccuracy: 58.68%\n",
            "\n",
            "Loss: 12927.07597\tAccuracy:53.76%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-91-a1e3bc24fb30>:98: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"Loss: %.5f\\tAccuracy:%.2f%%\" % (loss, accuracy*100))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "25c13297",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25c13297",
        "outputId": "f36cc0d0-625e-4d50-8039-09c67bfca879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-91-a1e3bc24fb30>:86: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"Epoch: %d\\tTime: %.2fs\\tLoss: %.5f\\tAccuracy: %.2f%%\" % (epoch+1, time()-start, loss_list[-1], accuracy_list[-1]*100))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3\tTime: 6.11s\tLoss: 1.53511\tAccuracy: 45.06%\n",
            "Epoch: 6\tTime: 5.52s\tLoss: 1.39775\tAccuracy: 50.10%\n",
            "Epoch: 9\tTime: 4.89s\tLoss: 1.31259\tAccuracy: 53.09%\n",
            "Epoch: 12\tTime: 5.00s\tLoss: 1.25346\tAccuracy: 54.94%\n",
            "Epoch: 15\tTime: 6.15s\tLoss: 1.21068\tAccuracy: 56.55%\n",
            "Epoch: 18\tTime: 5.03s\tLoss: 1.17362\tAccuracy: 57.97%\n",
            "Epoch: 21\tTime: 4.80s\tLoss: 1.14920\tAccuracy: 58.92%\n",
            "Epoch: 24\tTime: 5.72s\tLoss: 1.13008\tAccuracy: 59.74%\n",
            "Epoch: 27\tTime: 6.06s\tLoss: 1.10346\tAccuracy: 60.75%\n",
            "Epoch: 30\tTime: 4.76s\tLoss: 1.08555\tAccuracy: 61.23%\n",
            "Epoch: 33\tTime: 4.82s\tLoss: 1.06747\tAccuracy: 61.99%\n",
            "Epoch: 36\tTime: 4.84s\tLoss: 1.06553\tAccuracy: 62.02%\n",
            "Epoch: 39\tTime: 6.23s\tLoss: 1.05769\tAccuracy: 62.33%\n",
            "Epoch: 42\tTime: 5.70s\tLoss: 1.03491\tAccuracy: 63.13%\n",
            "Epoch: 45\tTime: 4.74s\tLoss: 1.02680\tAccuracy: 63.34%\n",
            "Epoch: 48\tTime: 4.70s\tLoss: 1.01816\tAccuracy: 63.88%\n",
            "\n",
            "Loss: 12587.93686\tAccuracy:55.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-91-a1e3bc24fb30>:98: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"Loss: %.5f\\tAccuracy:%.2f%%\" % (loss, accuracy*100))\n"
          ]
        }
      ],
      "source": [
        "optimizer = Optimizer(lr = 0.001, momentum = 0.8, weight_decay = 1e-3)\n",
        "\n",
        "n_in = X_train.shape[1]\n",
        "n_out = len(np.unique(y_train))\n",
        "\n",
        "layer = [256, 512]\n",
        "\n",
        "activation = ['relu', 'relu', 'softmax']\n",
        "model = MLP(n_in, n_out, layer, optimizer, activation, BN = True, Dropout = True, dropout_prob = [0.2, 0.2])\n",
        "\n",
        "loss, accuracy = model.fit(X_train, y_train, epochs = 50, batch_size = 1000, print_per = 3)\n",
        "\n",
        "print()\n",
        "\n",
        "model.predict(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0e12c59",
      "metadata": {
        "id": "b0e12c59"
      },
      "source": [
        "# 5. 预测(测试集)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5c818a3",
      "metadata": {
        "id": "e5c818a3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}