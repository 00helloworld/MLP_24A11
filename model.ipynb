{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "421365f2",
      "metadata": {
        "id": "421365f2"
      },
      "source": [
        "# 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4b155c87",
      "metadata": {
        "id": "4b155c87"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "from time import time\n",
        "\n",
        "X_train = np.load('train_data.npy')\n",
        "X_test = np.load('test_data.npy')\n",
        "y_train = np.load('train_label.npy')\n",
        "y_test = np.load('test_label.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ec8636bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec8636bc",
        "outputId": "896b85df-9b07-442e-f53c-ef9f2be4c586"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((50000, 128), (10000, 128), (50000, 1), (10000, 1))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c83d0ca",
      "metadata": {
        "id": "0c83d0ca"
      },
      "source": [
        "# 2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "HBAPZ6nGF_6_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBAPZ6nGF_6_",
        "outputId": "49971c0e-4a76-4c50-fc73-9c95af9dc0bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.0, 1.0000000000000002, -0.2341860850703863, 1.1540823219473646)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "train_data_scaled = scaler.fit_transform(X_train)\n",
        "test_data_scaled = scaler.transform(X_test)\n",
        "\n",
        "train_data_min, train_data_max = train_data_scaled.min(), train_data_scaled.max()\n",
        "test_data_min, test_data_max = test_data_scaled.min(), test_data_scaled.max()\n",
        "\n",
        "train_data_min, train_data_max, test_data_min, test_data_max\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "104d5337",
      "metadata": {
        "id": "104d5337"
      },
      "source": [
        "# 3. Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce68dda7",
      "metadata": {
        "id": "ce68dda7"
      },
      "source": [
        "## 3.1 Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daa1055a",
      "metadata": {},
      "source": [
        "<span style='color:green'>DONE</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b9bf35f8",
      "metadata": {
        "id": "b9bf35f8"
      },
      "outputs": [],
      "source": [
        "# Activation function class\n",
        "class Activation(object):\n",
        "    def __init__(self, activation='relu'):\n",
        "        # Supported activation functions and their derivatives\n",
        "        self.activations = {\n",
        "            'relu': (self.relu, self.relu_derivative),\n",
        "            'softmax': (self.softmax, None)\n",
        "        }\n",
        "        \n",
        "        self.set_activation(activation)\n",
        "\n",
        "    def set_activation(self, activation):\n",
        "        # Set the activation function and its derivative\n",
        "        if activation in self.activations:\n",
        "            self.function, self.function_derivative = self.activations[activation]\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation function: {activation}\")\n",
        "\n",
        "    def relu(self, x):\n",
        "        # ReLU activation function\n",
        "        relu = np.where(x >= 0, x, 0)\n",
        "        return relu\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        # Derivative of ReLU activation function\n",
        "        relu_de = np.where(x >= 0, 1, 0)\n",
        "        return relu_de\n",
        "\n",
        "    def softmax(self, x):\n",
        "        # Softmax activation function\n",
        "        x_exp = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        sm = x_exp / np.sum(x_exp, axis=-1, keepdims=True)\n",
        "        return sm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb256d68",
      "metadata": {
        "id": "bb256d68"
      },
      "source": [
        "## 3.2 Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5a9b894",
      "metadata": {},
      "source": [
        "<span style='color:green'>DONE</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7e87f565",
      "metadata": {
        "id": "7e87f565"
      },
      "outputs": [],
      "source": [
        "# Layer class\n",
        "class Layer(object):\n",
        "\n",
        "    def __init__(self, n_input, n_output, optimizer, activation='relu'):\n",
        "        self.input = None\n",
        "        self.logit = None\n",
        "        self.output = None\n",
        "\n",
        "        # Initialize activation function and its derivative\n",
        "        self.activation = Activation(activation).function\n",
        "        self.activation_derivative = Activation(activation).function_derivative if Activation(activation).function_derivative else None\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.Weight = self.initialize_weights(n_input, n_output)\n",
        "        self.bias = np.zeros(n_output,)\n",
        "\n",
        "        # Copy optimizer for weights and biases\n",
        "        self.optimizer_weight = copy.copy(optimizer)\n",
        "        self.optimizer_bias = copy.copy(optimizer)\n",
        "\n",
        "    def initialize_weights(self, n_input, n_output):\n",
        "        # Initialize weights using He initialization\n",
        "        limit = np.sqrt(6 / (n_input + n_output))\n",
        "        return np.random.uniform(low=-limit, high=limit, size=(n_input, n_output))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Forward pass through the layer\n",
        "        self.input = input\n",
        "        self.logit = np.dot(input, self.Weight) + self.bias\n",
        "        self.output = self.activation(self.logit)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, delta):\n",
        "        # Backward pass through the layer\n",
        "        if self.activation_derivative:\n",
        "            delta = delta * self.activation_derivative(self.logit)\n",
        "\n",
        "        grad_weight = np.dot(self.input.T, delta)\n",
        "        grad_bias = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.Weight = self.optimizer_weight.update(self.Weight, grad_weight)\n",
        "        self.bias = self.optimizer_bias.update(self.bias, grad_bias)\n",
        "\n",
        "        delta = np.dot(delta, self.Weight.T)\n",
        "\n",
        "        return delta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b8bd0c",
      "metadata": {
        "id": "28b8bd0c"
      },
      "source": [
        "## 3.3 Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "278331ca",
      "metadata": {},
      "source": [
        "<span style='color:green'>DONE</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "79930d24",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dropout layer class\n",
        "class DropoutLayer(object):\n",
        "\n",
        "    def __init__(self, dropout=0.5):\n",
        "        self.dropout = dropout\n",
        "        self.mask = None\n",
        "\n",
        "    def generate_mask(self, shape):\n",
        "        # Generate dropout mask\n",
        "        mask = np.random.rand(*shape) >= self.dropout\n",
        "        return mask\n",
        "\n",
        "    def apply_mask(self, X, mask):\n",
        "        # Apply dropout mask\n",
        "        return X * mask\n",
        "\n",
        "    def forward(self, X, train=True):\n",
        "        # Forward pass through dropout layer\n",
        "        if train:\n",
        "            self.mask = self.generate_mask(X.shape)\n",
        "            return self.apply_mask(X, self.mask)\n",
        "        else:\n",
        "            return self.apply_mask(X, 1 - self.dropout)\n",
        "\n",
        "    def backward(self, delta):\n",
        "        # Backward pass through dropout layer\n",
        "        update = delta * self.mask if self.mask is not None else delta\n",
        "        return update\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee80f568",
      "metadata": {
        "id": "ee80f568"
      },
      "source": [
        "## 3.4 BatchNormalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "292e445a",
      "metadata": {},
      "source": [
        "<span style='color:green'>DONE</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "889c7459",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch normalization class\n",
        "class BatchNormalization(object):\n",
        "\n",
        "    def __init__(self, gamma, beta, optimizer, momentum=0.95):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.mean = None\n",
        "        self.var = None\n",
        "\n",
        "        # Copy optimizer for gamma and beta\n",
        "        self.optimizer_gamma = copy.copy(optimizer)\n",
        "        self.optimizer_beta = copy.copy(optimizer)\n",
        "\n",
        "    def compute_mean_var(self, X):\n",
        "        # Compute mean and variance\n",
        "        mean = np.mean(X, axis=0)\n",
        "        var = np.var(X, axis=0)\n",
        "        return mean, var\n",
        "\n",
        "    def update_mean_var(self, mean, var):\n",
        "        # Update mean and variance using momentum\n",
        "        if self.mean is None or self.var is None:\n",
        "            self.mean = mean\n",
        "            self.var = var\n",
        "        else:\n",
        "            self.mean = self.momentum * self.mean + (1 - self.momentum) * mean\n",
        "            self.var = self.momentum * self.var + (1 - self.momentum) * var\n",
        "\n",
        "    def normalize(self, X):\n",
        "        # Normalize the input\n",
        "        self.X_minus_mean = X - self.mean\n",
        "        self.std = np.sqrt(self.var + 1e-6)\n",
        "        self.X_norm = self.X_minus_mean / self.std\n",
        "\n",
        "    def forward(self, X, train=True):\n",
        "        # Forward pass through batch normalization\n",
        "        if train:\n",
        "            mean, var = self.compute_mean_var(X)\n",
        "            self.update_mean_var(mean, var)\n",
        "        else:\n",
        "            mean = self.mean\n",
        "            var = self.var\n",
        "\n",
        "        self.normalize(X)\n",
        "        output = self.gamma * self.X_norm + self.beta\n",
        "        return output\n",
        "\n",
        "    def compute_gradients(self, delta):\n",
        "        # Compute gradients for gamma and beta\n",
        "        grad_gamma = np.sum(delta * self.X_norm, axis=0)\n",
        "        grad_beta = np.sum(delta, axis=0)\n",
        "\n",
        "        dX_norm = delta * self.gamma\n",
        "        dvar = np.sum(dX_norm * self.X_minus_mean, axis=0) * (-0.5) * (self.var + 1e-6)**(-3/2)\n",
        "        dmean = np.sum(dX_norm * (1/self.std), axis=0) + dvar * (1/delta.shape[0]) * np.sum(-2 * self.X_minus_mean, axis=0)\n",
        "        delta = (dX_norm * (1/self.std)) + (dmean / delta.shape[0]) + (dvar * 2 / delta.shape[0] * self.X_minus_mean)\n",
        "\n",
        "        return grad_gamma, grad_beta, delta\n",
        "\n",
        "    def backward(self, delta):\n",
        "        # Backward pass through batch normalization\n",
        "        gamma_grad, beta_grad, delta = self.compute_gradients(delta)\n",
        "\n",
        "        self.gamma = self.optimizer_gamma.update(self.gamma, gamma_grad)\n",
        "        self.beta = self.optimizer_beta.update(self.beta, beta_grad)\n",
        "\n",
        "        return delta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e030a362",
      "metadata": {
        "id": "e030a362"
      },
      "source": [
        "## 3.5 Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6d9c4d7",
      "metadata": {},
      "source": [
        "<span style='color:green'>DONE</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6000550d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizer class\n",
        "class Optimizer(object):\n",
        "\n",
        "    def __init__(self, lr=0.001, momentum=0.95, weight_decay=1e-2):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.weight_decay = weight_decay\n",
        "        self.velocity = None\n",
        "\n",
        "    def initialize_velocity(self, shape):\n",
        "        # Initialize velocity for momentum\n",
        "        self.velocity = np.zeros(shape)\n",
        "\n",
        "    def compute_velocity(self, delta):\n",
        "        # Compute velocity for momentum\n",
        "        self.velocity = self.momentum * self.velocity + (1 - self.momentum) * delta\n",
        "\n",
        "    def update_weight(self, weight):\n",
        "        # Update weights with weight decay and momentum\n",
        "        weight = weight * (1 - self.weight_decay) - self.lr * self.velocity\n",
        "        return weight\n",
        "\n",
        "    def update(self, weight, delta):\n",
        "        # Update weights using optimizer\n",
        "        if self.velocity is None:\n",
        "            self.initialize_velocity(weight.shape)\n",
        "        \n",
        "        self.compute_velocity(delta)\n",
        "        weight = self.update_weight(weight)\n",
        "\n",
        "        return weight\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fef8df57",
      "metadata": {
        "id": "fef8df57"
      },
      "source": [
        "## 3.6 Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cd4a862",
      "metadata": {},
      "source": [
        "<span style='color:green'>DONE</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c69346d5",
      "metadata": {
        "id": "c69346d5"
      },
      "outputs": [],
      "source": [
        "# MLP class\n",
        "class MLP(object):\n",
        "\n",
        "    def __init__(self, n_input, n_output, hidden_layers, optimizer, activation, BN=False, Dropout=False, dropout=None):\n",
        "\n",
        "        self.layers = []\n",
        "        self.activation = activation\n",
        "        self.optimizer = optimizer\n",
        "        self.lr = self.optimizer.lr\n",
        "        self.n_out = n_output\n",
        "\n",
        "        # Add layers to the model\n",
        "        self.add_layers(n_input, hidden_layers, BN, Dropout, dropout)\n",
        "\n",
        "    def add_layers(self, n_input, hidden_layers, BN, Dropout, dropout):\n",
        "        # Add layers with optional Batch Normalization and Dropout\n",
        "        for i, (layer_size, activation) in enumerate(zip(hidden_layers, self.activation)):\n",
        "            self.layers.append(Layer(n_input if i == 0 else hidden_layers[i-1], layer_size, self.optimizer, activation))\n",
        "            \n",
        "            if Dropout:\n",
        "                self.layers.append(DropoutLayer(dropout[i]))\n",
        "            if BN:\n",
        "                self.layers.append(BatchNormalization(np.ones((1, layer_size)), np.zeros((1, layer_size)), self.optimizer))\n",
        "\n",
        "        self.layers.append(Layer(hidden_layers[-1], self.n_out, self.optimizer, self.activation[-1]))\n",
        "\n",
        "    def CE_loss(self, y, predict_y):\n",
        "        # Compute cross-entropy loss and its gradient\n",
        "        y_onehot = np.eye(self.n_out)[y].reshape(-1, self.n_out)\n",
        "        predict_y = np.clip(predict_y, 1e-15, 1 - 1e-15)\n",
        "        loss = -np.sum(np.multiply(y_onehot, np.log(predict_y)))\n",
        "        delta = predict_y - y_onehot\n",
        "        return loss, delta\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Forward pass through the network\n",
        "        output = input\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output)\n",
        "        return output\n",
        "\n",
        "    def backward(self, delta):\n",
        "        # Backward pass through the network\n",
        "        for layer in reversed(self.layers):\n",
        "            delta = layer.backward(delta)\n",
        "\n",
        "    def fit(self, X, y, epochs=100, batch_size=100, print_per=50):\n",
        "        # Training method\n",
        "        loss_list = []\n",
        "        accuracy_list = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.update_lr(epoch, epochs)\n",
        "\n",
        "            loss_sum = 0\n",
        "            predict_y_batch = []\n",
        "\n",
        "            start = time()\n",
        "\n",
        "            for i in np.arange(0, X.shape[0], batch_size):\n",
        "                X_batch = X[i: min(i+batch_size, X.shape[0])]\n",
        "                y_batch = y[i: min(i+batch_size, X.shape[0])]\n",
        "\n",
        "                predict_y = self.forward(X_batch)\n",
        "\n",
        "                loss, delta = self.CE_loss(y_batch, predict_y)\n",
        "\n",
        "                self.backward(delta)\n",
        "\n",
        "                loss_sum += loss\n",
        "                predict_y_batch.extend(predict_y)\n",
        "\n",
        "            predict_y_batch = np.array(predict_y_batch)\n",
        "\n",
        "            loss_list.append(loss_sum / X.shape[0])\n",
        "            predict_y = np.argmax(predict_y_batch, axis=1).reshape(-1, 1)\n",
        "            accuracy = np.sum(predict_y == y, axis=0) / X.shape[0]\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if (epoch + 1) % print_per == 0:\n",
        "                print(\"Epoch: %d\\tTime: %.2fs\\tLoss: %.5f\\tAccuracy: %.2f%%\" % (epoch+1, time()-start, loss_list[-1], accuracy_list[-1]*100))\n",
        "\n",
        "        return loss_list, accuracy_list\n",
        "\n",
        "    def update_lr(self, epoch, total_epochs):\n",
        "        # Update learning rate at specific epochs\n",
        "        if epoch == int(total_epochs*1/3) or epoch == int(total_epochs*2/3):\n",
        "            self.lr /= 5\n",
        "            self.optimizer = Optimizer(lr=self.lr)\n",
        "\n",
        "    def predict(self, X, y):\n",
        "        # Prediction method\n",
        "        predict_y = self.forward(X)\n",
        "\n",
        "        loss, _ = self.CE_loss(y, predict_y)\n",
        "        accuracy = np.sum(np.argmax(predict_y, axis=1).reshape(-1, 1) == y, axis=0) / X.shape[0]\n",
        "\n",
        "        print(\"Loss: %.5f\\tAccuracy:%.2f%%\" % (loss, accuracy*100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0eff823",
      "metadata": {
        "id": "c0eff823"
      },
      "source": [
        "# 4. Adjust Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "vFXWm15fIlN2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFXWm15fIlN2",
        "outputId": "39055ef1-8a07-4ac1-90c7-b5713e2ba0bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hk/p7fcnhm10z92trzs_9kwqlyr0000gn/T/ipykernel_93656/2408756749.py:81: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"Epoch: %d\\tTime: %.2fs\\tLoss: %.5f\\tAccuracy: %.2f%%\" % (epoch+1, time()-start, loss_list[-1], accuracy_list[-1]*100))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3\tTime: 3.62s\tLoss: 1.59170\tAccuracy: 42.71%\n",
            "Epoch: 6\tTime: 4.14s\tLoss: 1.45648\tAccuracy: 47.42%\n",
            "Epoch: 9\tTime: 3.35s\tLoss: 1.37713\tAccuracy: 50.65%\n",
            "Epoch: 12\tTime: 3.50s\tLoss: 1.31893\tAccuracy: 52.56%\n",
            "Epoch: 15\tTime: 3.88s\tLoss: 1.27322\tAccuracy: 54.19%\n",
            "Epoch: 18\tTime: 3.74s\tLoss: 1.23240\tAccuracy: 56.08%\n",
            "Epoch: 21\tTime: 3.74s\tLoss: 1.20525\tAccuracy: 56.85%\n",
            "Epoch: 24\tTime: 4.00s\tLoss: 1.19056\tAccuracy: 57.22%\n",
            "Epoch: 27\tTime: 3.94s\tLoss: 1.17502\tAccuracy: 58.19%\n",
            "Epoch: 30\tTime: 3.70s\tLoss: 1.14216\tAccuracy: 59.31%\n",
            "Epoch: 33\tTime: 3.89s\tLoss: 1.14264\tAccuracy: 59.00%\n",
            "Epoch: 36\tTime: 3.62s\tLoss: 1.13081\tAccuracy: 59.45%\n",
            "Epoch: 39\tTime: 3.88s\tLoss: 1.11868\tAccuracy: 60.23%\n",
            "Epoch: 42\tTime: 3.94s\tLoss: 1.09958\tAccuracy: 60.40%\n",
            "Epoch: 45\tTime: 4.67s\tLoss: 1.08211\tAccuracy: 61.53%\n",
            "Epoch: 48\tTime: 5.71s\tLoss: 1.07053\tAccuracy: 61.71%\n",
            "Epoch: 51\tTime: 3.68s\tLoss: 1.06033\tAccuracy: 62.15%\n",
            "Epoch: 54\tTime: 5.42s\tLoss: 1.05035\tAccuracy: 62.42%\n",
            "Epoch: 57\tTime: 5.62s\tLoss: 1.06445\tAccuracy: 62.05%\n",
            "Epoch: 60\tTime: 4.63s\tLoss: 1.05173\tAccuracy: 62.30%\n",
            "Epoch: 63\tTime: 6.16s\tLoss: 1.04900\tAccuracy: 62.50%\n",
            "Epoch: 66\tTime: 4.21s\tLoss: 1.05292\tAccuracy: 62.38%\n",
            "Epoch: 69\tTime: 4.86s\tLoss: 1.05379\tAccuracy: 62.28%\n",
            "Epoch: 72\tTime: 4.48s\tLoss: 1.03991\tAccuracy: 62.71%\n",
            "Epoch: 75\tTime: 4.36s\tLoss: 1.02888\tAccuracy: 63.26%\n",
            "Epoch: 78\tTime: 3.41s\tLoss: 1.02620\tAccuracy: 63.41%\n",
            "Epoch: 81\tTime: 3.46s\tLoss: 1.01751\tAccuracy: 63.74%\n",
            "Epoch: 84\tTime: 3.50s\tLoss: 1.03387\tAccuracy: 63.08%\n",
            "Epoch: 87\tTime: 3.36s\tLoss: 1.02154\tAccuracy: 63.62%\n",
            "Epoch: 90\tTime: 3.53s\tLoss: 1.00994\tAccuracy: 63.86%\n",
            "Epoch: 93\tTime: 3.95s\tLoss: 1.01122\tAccuracy: 64.01%\n",
            "Epoch: 96\tTime: 6.13s\tLoss: 0.99216\tAccuracy: 64.47%\n",
            "Epoch: 99\tTime: 4.34s\tLoss: 1.00734\tAccuracy: 64.17%\n",
            "\n",
            "Loss: 14052.00632\tAccuracy:52.16%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hk/p7fcnhm10z92trzs_9kwqlyr0000gn/T/ipykernel_93656/2408756749.py:98: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"Loss: %.5f\\tAccuracy:%.2f%%\" % (loss, accuracy*100))\n"
          ]
        }
      ],
      "source": [
        "# Initialize optimizer\n",
        "optimizer = Optimizer(lr=0.001, momentum=0.8, weight_decay=1e-3)\n",
        "\n",
        "n_input = X_train.shape[1]\n",
        "n_output = len(np.unique(y_train))\n",
        "\n",
        "layer2 = [256, 512, 256]\n",
        "\n",
        "activation = ['relu', 'relu', 'relu', 'softmax']\n",
        "model = MLP(n_input, n_output, layer2, optimizer, activation, BN=True, Dropout=True, dropout=[0.2, 0.2, 0.2])\n",
        "\n",
        "# Train the model\n",
        "loss, accuracy = model.fit(X_train, y_train, epochs=100, batch_size=1000, print_per=3)\n",
        "\n",
        "print()\n",
        "\n",
        "# Test the model\n",
        "model.predict(X_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
